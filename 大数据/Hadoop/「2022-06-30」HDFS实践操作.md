```text
HDFS实践操作
大数据>Hadoop
2022-06-30
https://picgo.kwcoder.club/202206/202206261620161.png
```



## 目录操作

- `-ls`：显示目录信息

```shell
[root@master ~]# hdfs dfs -ls /
2022-07-08 10:26:13,603 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
drwxr-xr-x   - dr.who supergroup          0 2022-07-07 22:05 /demo
drwx------   - root   supergroup          0 2022-07-07 22:05 /tmp
drwxr-xr-x   - root   supergroup          0 2022-07-07 22:06 /wc
```

- `-mkdir`：在HDFS上创建目录

```shell
[root@master ~]# hdfs dfs -mkdir /new_dir
2022-07-08 10:27:07,478 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@master ~]# hdfs dfs -ls /
2022-07-08 10:27:14,808 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 4 items
drwxr-xr-x   - dr.who supergroup          0 2022-07-07 22:05 /demo
drwxr-xr-x   - root   supergroup          0 2022-07-08 10:27 /new_dir
drwx------   - root   supergroup          0 2022-07-07 22:05 /tmp
drwxr-xr-x   - root   supergroup          0 2022-07-07 22:06 /wc
```

- `-rm`：删除HDFS上的文件或目录

```shell
[root@master ~]# hdfs dfs -rm -r /new_dir 
2022-07-08 10:56:04,807 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Deleted /new_dir
[root@master ~]# hdfs dfs -ls /
2022-07-08 10:56:18,549 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
drwxr-xr-x   - dr.who supergroup          0 2022-07-07 22:05 /demo
drwx------   - root   supergroup          0 2022-07-07 22:05 /tmp
drwxr-xr-x   - root   supergroup          0 2022-07-07 22:06 /wc
```

同时，HDFS也支持移动、重命名、更改所属用户、所数组等，其操作和Linux系统类似，命令为`hadoop dfs -命令 文件`

## 文件操作

`-cat`：查看文件内容

```shell
[root@master ~]# hdfs dfs -cat /new_dir/test.txt
2022-07-08 10:44:11,740 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
I lova China!
```

`-appendToFile`：追加一个文件到已经存在的文件末尾（从本地）

```shell
[root@master ~]# hdfs dfs -appendToFile ./test.txt /new_dir/test1.txt
2022-07-08 10:46:03,378 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@master ~]# hdfs dfs -cat /new_dir/test1.txt
2022-07-08 10:46:19,150 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
I love China!
I lova China!
```

## 上传操作

- `-moveFromLocal`：从本地剪切到HDFS

步骤与解释：

1. 创建一个文件`test.txt`，编辑内容为`I lova China`
2. 执行`-moveFromLocal`
3. 查看HDFS上的文件和本地的文件。HDFS上存在该文件，但是本地不存在，原因是该操作是剪切。

```shell
[root@master ~]# vim test.txt
[root@master ~]# hdfs dfs -moveFromLocal ./test.txt /new_dir
2022-07-08 10:30:49,932 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@master ~]# hdfs dfs -ls /new_dir
2022-07-08 10:31:06,558 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rw-r--r--   2 root supergroup         14 2022-07-08 10:30 /new_dir/test.txt
[root@master ~]# ls
anaconda-ks.cfg
```

- `-copyFromLocal`：从本地剪切到HDFS

步骤与解释：

1. 创建一个文件`test1.txt`，编辑内容为`I lova China`
2. 执行`-copyFromLocal`
3. 查看HDFS上的文件和本地的文件。HDFS上存在该文件，本地也存在，原因是该操作是复制。

```shell
[root@master ~]# vim test1.txt
[root@master ~]# hdfs dfs -copyFromLocal ./test1.txt /new_dir
2022-07-08 10:33:57,043 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@master ~]# hdfs dfs -ls /new_dir
2022-07-08 10:34:09,978 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   2 root supergroup         14 2022-07-08 10:30 /new_dir/test.txt
-rw-r--r--   2 root supergroup         14 2022-07-08 10:33 /new_dir/test1.txt
[root@master ~]# ls
anaconda-ks.cfg  test1.txt
```

- `-put`：等同于`-copyFromLocal`

```shell
[root@master ~]# hdfs dfs -ls /new_dir
2022-07-08 10:50:32,215 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
-rw-r--r--   2 root supergroup         14 2022-07-08 10:30 /new_dir/test.txt
-rw-r--r--   2 root supergroup         28 2022-07-08 10:46 /new_dir/test1.txt
-rw-r--r--   2 root supergroup         14 2022-07-08 10:50 /new_dir/test2.txt
```

## 下载操作

- `-copyToLocal`：从 HDFS 拷贝到本地

```shell
[root@master ~]# hdfs dfs -copyToLocal /new_dir/test.txt
2022-07-08 10:39:40,697 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@master ~]# ls
anaconda-ks.cfg  test.txt
```

- `-get`：等同于 copyToLocal，就是从 HDFS 下载文件到本地

```shell
[root@master ~]# hdfs dfs -get /new_dir/test1.txt
2022-07-08 10:40:30,199 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@master ~]# ls
anaconda-ks.cfg  test.txt  test1.txt
```

- `-getmerge`：合并下载多个文件

```shell
[root@master ~]# hdfs dfs -getmerge /new_dir/* ./new_test.txt
2022-07-08 10:52:13,594 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@master ~]# cat ./new_test.txt 
I lova China!
I love China!
I lova China!
I lova China!
```

## 集群管理

- `hdfs dfsadmin -report`：显示集群状态

```shell
[root@master ~]# hdfs dfsadmin -report
2022-07-08 11:02:16,255 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Configured Capacity: 125451247616 (116.84 GB)
Present Capacity: 103752818688 (96.63 GB)
DFS Remaining: 103752097792 (96.63 GB)
DFS Used: 720896 (704 KB)
DFS Used%: 0.00%
Replicated Blocks:
	Under replicated blocks: 0
	Blocks with corrupt replicas: 0
	Missing blocks: 0
	Missing blocks (with replication factor 1): 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0
Erasure Coded Block Groups: 
	Low redundancy block groups: 0
	Block groups with corrupt internal blocks: 0
	Missing block groups: 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (2):

Name: 172.18.0.3:9866 (slave1.hadoop_test)
Hostname: slave1
Decommission Status : Normal
Configured Capacity: 62725623808 (58.42 GB)
DFS Used: 360448 (352 KB)
Non DFS Used: 7632498688 (7.11 GB)
DFS Remaining: 51876048896 (48.31 GB)
DFS Used%: 0.00%
DFS Remaining%: 82.70%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Fri Jul 08 11:02:18 CST 2022
Last Block Report: Fri Jul 08 09:36:12 CST 2022
Num of Blocks: 4


Name: 172.18.0.4:9866 (slave2.hadoop_test)
Hostname: slave2
Decommission Status : Normal
Configured Capacity: 62725623808 (58.42 GB)
DFS Used: 360448 (352 KB)
Non DFS Used: 7632498688 (7.11 GB)
DFS Remaining: 51876048896 (48.31 GB)
DFS Used%: 0.00%
DFS Remaining%: 82.70%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Fri Jul 08 11:02:18 CST 2022
Last Block Report: Fri Jul 08 08:59:39 CST 2022
Num of Blocks: 4
```

- `hdfs dfsadmin -printTopology`：显示集群拓扑结构

```shell
[root@master ~]# hdfs dfsadmin -printTopology
2022-07-08 11:03:04,657 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Rack: /default-rack
   172.18.0.3:9866 (slave1.hadoop_test)
   172.18.0.4:9866 (slave2.hadoop_test)
```

- `hdfs dfsadmin -safemode get`：查看安全模式状态

```shell
[root@master ~]# hdfs dfsadmin -safemode get
2022-07-08 11:03:45,298 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Safe mode is OFF
```

- `hdfs dfsadmin -safemode leave`：退出安全模式

```shell
[root@master ~]# hdfs dfsadmin -safemode leave
2022-07-08 11:04:01,402 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Safe mode is OFF
```

## 文件检查

- `hdfs fsck 文件名`

```shell
[root@master ~]# hdfs fsck /demo/test.txt
2022-07-08 11:09:01,327 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Connecting to namenode via http://master:50070/fsck?ugi=root&path=%2Fdemo%2Ftest.txt
FSCK started by root (auth:SIMPLE) from /172.18.0.2 for path /demo/test.txt at Fri Jul 08 11:09:03 CST 2022


Status: HEALTHY
 Number of data-nodes:	2
 Number of racks:		1
 Total dirs:			0
 Total symlinks:		0

Replicated Blocks:
 Total size:	60 B
 Total files:	1
 Total blocks (validated):	1 (avg. block size 60 B)
 Minimally replicated blocks:	1 (100.0 %)
 Over-replicated blocks:	0 (0.0 %)
 Under-replicated blocks:	0 (0.0 %)
 Mis-replicated blocks:		0 (0.0 %)
 Default replication factor:	2
 Average block replication:	2.0
 Missing blocks:		0
 Corrupt blocks:		0
 Missing replicas:		0 (0.0 %)
 Blocks queued for replication:	0

Erasure Coded Block Groups:
 Total size:	0 B
 Total files:	0
 Total block groups (validated):	0
 Minimally erasure-coded block groups:	0
 Over-erasure-coded block groups:	0
 Under-erasure-coded block groups:	0
 Unsatisfactory placement block groups:	0
 Average block group size:	0.0
 Missing block groups:		0
 Corrupt block groups:		0
 Missing internal blocks:	0
 Blocks queued for replication:	0
FSCK ended at Fri Jul 08 11:09:03 CST 2022 in 2 milliseconds


The filesystem under path '/demo/test.txt' is HEALTHY
```

# HDFS WEB 接口

[http://master:50070](http://master:50070)

![1hdfsweb接口.png](/images/大数据/5hdfs操作/1hdfsweb接口.png)

# HDFS 编程开发（Java）

> 前置条件：本地拥有Java、Maven、Hadoop环境

## 需要添加的`Maven`依赖：

```xml
<dependencies>

    <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.2</version>
    </dependency>

    <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.2</version>
    </dependency>

    <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>3.3.2</version>
        <scope>test</scope>
    </dependency>

</dependencies>
```

## 获取HDFS文件系统

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;

import java.io.IOException;

public class Demo {

    public static void main(String[] args) {

        try {
            // 构建配置对象
            Configuration conf = new Configuration();

            // 配置属性（如果提供Hadoop中的core-site.xml和hdfs-site.xml配置文件时，会自动读取集群的配置，无需设置该项）
            conf.set("fs.defaultFS", "hdfs://master:9000");

            // 获取HDFS对象
            FileSystem hdfs = FileSystem.get(conf);

            // 关闭资源
            hdfs.close();
        } catch (IOException e) {
            e.printStackTrace();
        }

    }

}
```

## 创建、上传、下载、重命名的操作

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;

import java.io.IOException;

public class Demo {
    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            FileSystem hdfs = FileSystem.get(conf);
            
            // 创建文件夹
            hdfs.mkdirs(new Path("/test"));
            // 上传文件，参数：原文件地址（需要上传的），目标地址
            hdfs.copyFromLocalFile(new Path("src/main/resources/"), new Path("/test"));
            // 下载文件，参数：原文件地址（需要下载的），目标地址
            hdfs.copyToLocalFile(new Path("/test/resources/"), new Path("/src/main/resources/"));
            // 删除文件/文件夹，参数：需要删除的文件/目录，是否递归删除
            hdfs.delete(new Path("/test"), true);
            // 重命名：原文件，重命名后的文件名
            hdfs.rename(new Path("/test/resources"), new Path("/test/resources1"));
            
            hdfs.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```

## 查看文件详情和判断的操作

```java
package com.neuedu.hdfs_demo;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;

import java.io.IOException;

public class Demo {
    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            FileSystem hdfs = FileSystem.get(conf);

            RemoteIterator<LocatedFileStatus> listFiles = hdfs.listFiles(new Path("/"), true);
            while (listFiles.hasNext()) {
                LocatedFileStatus next = listFiles.next();
                // 输出文件名
                System.out.println(next.getPath().getName());
                // 输出文件长度
                System.out.println(next.getLen());
                // 输出文件权限
                System.out.println(next.getPermission());
                // 输出文件所有组
                System.out.println(next.getGroup());
                // 获取存储的块信息
                BlockLocation[] blockLocations = next.getBlockLocations();
                for (BlockLocation blockLocation : blockLocations) {
                    // 获取块存储的主机节点
                    String[] hosts = blockLocation.getHosts();
                    for (String host : hosts) {
                        System.out.print(host + "\t");
                    }
                }
            }

            // ==========================

            FileStatus[] fileStatuses = hdfs.listStatus(new Path("/"));
            for (FileStatus fileStatus : fileStatuses) {
                // 输出是否为文件
                System.out.println(fileStatus.isFile());
            }

            hdfs.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```

## HDFS的I/O操作

> 建议阅读：[【Java 基础】IO 流](/p/20220705/)

### 文件上传

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.io.IOUtils;

import java.io.FileInputStream;
import java.io.IOException;

public class Demo {
    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            FileSystem hdfs = FileSystem.get(conf);

            // 获取输入流
            FileInputStream fis = new FileInputStream("src/main/resources/test.txt");
            // 获取输出流
            FSDataOutputStream fsdos = hdfs.create(new Path("/test.txt"));

            IOUtils.copyBytes(fis, fsdos, conf);
            IOUtils.closeStream(fsdos);
            IOUtils.closeStream(fis);

            hdfs.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```

### 文件下载

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.io.IOUtils;

import java.io.FileOutputStream;
import java.io.IOException;

public class Demo {
    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            FileSystem hdfs = FileSystem.get(conf);

            // 获取输入流
            FSDataInputStream fsdis = hdfs.open(new Path("/test.txt"));
            // 获取输出流
            FileOutputStream fos = new FileOutputStream("src/main/resources/test.txt");

            IOUtils.copyBytes(fsdis, fos, conf);
            IOUtils.closeStream(fos);
            IOUtils.closeStream(fsdis);

            hdfs.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

```

### 定位文件读取：分块下载文件

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.io.IOUtils;

import java.io.FileOutputStream;
import java.io.IOException;

public class Demo1 {

    public static void main(String[] args) {

        try {
            Configuration conf = new Configuration();
            FileSystem hdfs = FileSystem.get(conf);

            // 获取输入流
            FSDataInputStream fsdis = hdfs.open(new Path("/hadoop-3.3.1.tar.gz"));
            // 获取输出流
            FileOutputStream fos1 = new FileOutputStream("/src/main/resources/hadoop-3.3.1.tar.gz.part1");

            // 先拷贝0-128MB的片段
            byte[] bytes = new byte[1024];

            for (int i = 0; i < 1024 * 1024 * 128; i++) {
                fsdis.read(bytes);
                fos1.write(bytes);
            }

            IOUtils.closeStream(fos1);

            // 拷贝128MB之后的内容
            // 定位到已传输的位置
            fsdis.seek(1024 * 1024 * 128);
            // 获取输出流
            FileOutputStream fos2 = new FileOutputStream("/src/main/resources/hadoop-3.3.1.tar.gz.part2");
            // 流的对拷
            IOUtils.copyBytes(fsdis, fos2, conf);

            IOUtils.closeStream(fos2);
            IOUtils.closeStream(fsdis);

            hdfs.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```







